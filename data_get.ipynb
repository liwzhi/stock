{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#import statsmodels.tsa.seasonal as smt\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random\n",
    "import datetime as dt\n",
    "from sklearn import linear_model \n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "import os\n",
    "data_path = \"/notebooks/stock_data_analysis/data_2/Stocks\"\n",
    "os.chdir(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks/stock_data_analysis/data_2/Stocks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "# kernels let us navigate through the zipfile as if it were a directory\n",
    "\n",
    "# trying to read a file of size zero will throw an error, so skip them\n",
    "filenames = [x for x in os.listdir(data_path) if x.endswith('.txt') and os.path.getsize(x) > 0]\n",
    "# = random.sample(filenames,1)\n",
    "#print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def data_normalize(df, col):\n",
    "    data_close = np.reshape(df[col], (df[col].shape[0], 1))\n",
    "    scaler = MinMaxScaler()\n",
    "    result_get = scaler.fit_transform(data_close)\n",
    "    return result_get\n",
    "\n",
    "def binning_data(data, num_embedding = 1000):\n",
    "    bins = np.linspace(0, 1, num_embedding)\n",
    "    inds = np.digitize(data_get, bins)\n",
    "    return inds\n",
    "\n",
    "\n",
    "def get_data(file_name):\n",
    "    file_name = os.path.join('/notebooks/stock_data_analysis/data_2/Stocks', file_name)\n",
    "    df = pd.read_csv(file_name, sep=',')\n",
    "    #combine attributes\n",
    "    #combine high and low by avg\n",
    "    #combine open and close by avg\n",
    "    #combine avgHighLow and avgOpenClose\n",
    "    df['Price'] = (df['High'] + df['Low'] + df['Open'] + df['Close'])/4\n",
    "    #take log as this flattens the data more, resulting in a better prediction\n",
    "    df['Price'] = np.log(df['Price'])\n",
    "\n",
    "    #drop obsolete columns for faster processing\n",
    "    columns2Drop = [] #['High', 'Low', 'Open', 'Close', 'OpenInt']\n",
    "    df = df.drop(labels=columns2Drop, axis=1)\n",
    "\n",
    "    #create new attribute of \"movement\"\n",
    "    df['Volume*Price'] = df['Volume'] * df['Price']\n",
    "    # print(df)\n",
    "\n",
    "    label = filename\n",
    "    df['Label'] = label\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    #conver data to an number so we can fit it to LinearRegression()\n",
    "#     df['Date'] = df['Date'].map(dt.datetime.toordinal)\n",
    "\n",
    "    #data.append(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(data_path):\n",
    "    with open(data_path, 'rb') as handle:\n",
    "        data_get = pickle.load(handle)\n",
    "    return data_get\n",
    "\n",
    "def save_picle(data_path, data):\n",
    "    with open(data_path, 'wb') as handle:\n",
    "        pickle.dump(data, handle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(path):\n",
    "    \"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\n",
    "    try:\n",
    "        st = os.stat(path)\n",
    "    except os.error:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathModel = os.getcwd()  #\n",
    "embedding_col_path = os.path.join(pathModel, \"data_get.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = exists(embedding_col_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not flag:\n",
    "    data = []\n",
    "    for filename in filenames:\n",
    "        df = get_data(filename)\n",
    "        binning_sentence = data_normalize(df, 'Close')\n",
    "        inds = list(binning_data(binning_sentence).flat)\n",
    "        data.append(inds)\n",
    "    with open(embedding_col_path, 'w') as f:\n",
    "        f.write(json.dumps(data))\n",
    "else:\n",
    "    #Now read the file back into a Python list object\n",
    "    with open(embedding_col_path, 'r') as f:\n",
    "        data = json.loads(f.read())  \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 256\n",
    "embedding_size = 64\n",
    "generations = 50000\n",
    "print_loss_every = 500\n",
    "num_embedding = 1000\n",
    "num_sampled = int(batch_size/2)    # Number of negative examples to sample.\n",
    "window_size = 8       # How many words to consider left and right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_examples = [1,20,30,149,293]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data randomly (N words behind, target, N words ahead)\n",
    "def generate_batch_data(sentences, batch_size, window_size, method='skip_gram'):\n",
    "    # Fill up data batch\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    while len(batch_data) < batch_size:\n",
    "        # select random sentence to start\n",
    "        idx = np.random.choice(len(sentences),1)\n",
    "        rand_sentence = sentences[idx[0]]\n",
    "        # Generate consecutive windows to look at\n",
    "        window_sequences = [rand_sentence[max((ix-window_size),0):(ix+window_size+1)] for ix, x in enumerate(rand_sentence)]\n",
    "        # Denote which element of each window is the center word of interest\n",
    "        label_indices = [ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n",
    "        \n",
    "        # Pull out center word of interest for each window and create a tuple for each window\n",
    "        if method=='skip_gram':\n",
    "            batch_and_labels = [(x[y], x[:y] + x[(y+1):]) for x,y in zip(window_sequences, label_indices)]\n",
    "            # Make it in to a big list of tuples (target word, surrounding word)\n",
    "            tuple_data = [(x, y_) for x,y in batch_and_labels for y_ in y]\n",
    "        elif method=='cbow':\n",
    "            batch_and_labels = [(x[:y] + x[(y+1):], x[y]) for x,y in zip(window_sequences, label_indices)]\n",
    "            # Make it in to a big list of tuples (target word, surrounding word)\n",
    "            tuple_data = [(x_, y) for x,y in batch_and_labels for x_ in x]\n",
    "        else:\n",
    "            raise ValueError('Method {} not implemented yet.'.format(method))\n",
    "            \n",
    "        # extract batch and labels\n",
    "        batch, labels = [list(x) for x in zip(*tuple_data)]\n",
    "        batch_data.extend(batch[:batch_size])\n",
    "        label_data.extend(labels[:batch_size])\n",
    "    # Trim batch and label at the end\n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array([label_data]).astype(int))\n",
    "    \n",
    "    return(batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Embeddings:\n",
    "embeddings = tf.Variable(tf.random_uniform([num_embedding, embedding_size], -1.0, 1.0), name = \"product_vec\")\n",
    "\n",
    "# NCE loss parameters\n",
    "nce_weights = tf.Variable(tf.truncated_normal([num_embedding, embedding_size],\n",
    "                                               stddev=1.0 / np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([num_embedding]))\n",
    "\n",
    "# Create data/target placeholders\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "# Lookup the word embedding:\n",
    "embed = tf.nn.embedding_lookup(embeddings, x_inputs)\n",
    "\n",
    "# Get loss from prediction\n",
    "loss_get = tf.nn.nce_loss(weights=nce_weights,            # Tensor of shape(50000, 128)\n",
    "                          biases=nce_biases,              # vector of zeros; len(128)\n",
    "                          labels=y_target,            # labels == context words enums\n",
    "                          inputs=embed,                   # Tensor of shape(128, 128)\n",
    "                          num_sampled=num_sampled,        # 64: randomly chosen negative (rare) words\n",
    "                          num_classes=num_embedding)   # 50000: by construction\n",
    "loss = tf.reduce_mean(loss_get)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "\n",
    "\n",
    "try:\n",
    "    sess = tf.Session(config=config)\n",
    "except:\n",
    "    sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add variable initializer.\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 500 : 3.65587568283\n",
      "Loss at step 1000 : 2.57884311676\n",
      "Loss at step 1500 : 2.85987019539\n",
      "Loss at step 2000 : 3.23281097412\n",
      "Loss at step 2500 : 3.13484477997\n",
      "Loss at step 3000 : 2.91896104813\n",
      "Loss at step 3500 : 2.39592671394\n",
      "Loss at step 4000 : 2.40252494812\n",
      "Loss at step 4500 : 3.66019916534\n",
      "Loss at step 5000 : 3.7126531601\n",
      "Loss at step 5500 : 2.94248819351\n",
      "Loss at step 6000 : 3.1342253685\n",
      "Loss at step 6500 : 3.12881588936\n",
      "Loss at step 7000 : 2.5771727562\n",
      "Loss at step 7500 : 2.55633068085\n",
      "Loss at step 8000 : 2.47573137283\n",
      "Loss at step 8500 : 2.27930116653\n",
      "Loss at step 9000 : 2.24365377426\n",
      "Loss at step 9500 : 2.67922139168\n",
      "Loss at step 10000 : 2.43516778946\n",
      "Loss at step 10500 : 3.72865438461\n",
      "Loss at step 11000 : 2.97700190544\n",
      "Loss at step 11500 : 2.35210156441\n",
      "Loss at step 12000 : 2.97778701782\n",
      "Loss at step 12500 : 2.36637306213\n",
      "Loss at step 13000 : 2.29116225243\n",
      "Loss at step 13500 : 2.32501959801\n",
      "Loss at step 14000 : 2.72837233543\n",
      "Loss at step 14500 : 3.96937322617\n",
      "Loss at step 15000 : 2.73491358757\n",
      "Loss at step 15500 : 2.49873113632\n",
      "Loss at step 16000 : 3.24963235855\n",
      "Loss at step 16500 : 2.19588685036\n",
      "Loss at step 17000 : 2.59181046486\n",
      "Loss at step 17500 : 2.56394982338\n",
      "Loss at step 18000 : 3.74924373627\n",
      "Loss at step 18500 : 2.54179120064\n",
      "Loss at step 19000 : 2.67677569389\n",
      "Loss at step 19500 : 3.01445388794\n",
      "Loss at step 20000 : 2.2157125473\n",
      "Loss at step 20500 : 2.82489681244\n",
      "Loss at step 21000 : 2.73832249641\n",
      "Loss at step 21500 : 2.16809606552\n",
      "Loss at step 22000 : 3.07673835754\n",
      "Loss at step 22500 : 2.72987699509\n",
      "Loss at step 23000 : 3.02579045296\n",
      "Loss at step 23500 : 2.70468044281\n",
      "Loss at step 24000 : 3.68424224854\n",
      "Loss at step 24500 : 2.61665225029\n",
      "Loss at step 25000 : 2.68551492691\n",
      "Loss at step 25500 : 3.23497724533\n",
      "Loss at step 26000 : 2.5394089222\n",
      "Loss at step 26500 : 2.73664927483\n",
      "Loss at step 27000 : 2.23339295387\n",
      "Loss at step 27500 : 2.35712242126\n",
      "Loss at step 28000 : 2.29405832291\n",
      "Loss at step 28500 : 2.67611265182\n",
      "Loss at step 29000 : 2.28261113167\n",
      "Loss at step 29500 : 2.68531823158\n",
      "Loss at step 30000 : 2.52795886993\n",
      "Loss at step 30500 : 2.65092301369\n",
      "Loss at step 31000 : 2.71985912323\n",
      "Loss at step 31500 : 2.09455728531\n",
      "Loss at step 32000 : 2.28114938736\n",
      "Loss at step 32500 : 2.14243412018\n",
      "Loss at step 33000 : 2.50940322876\n",
      "Loss at step 33500 : 2.49853134155\n",
      "Loss at step 34000 : 2.19084405899\n",
      "Loss at step 34500 : 3.10253715515\n",
      "Loss at step 35000 : 2.45093822479\n",
      "Loss at step 35500 : 2.34334874153\n",
      "Loss at step 36000 : 2.39112663269\n",
      "Loss at step 36500 : 2.13879203796\n",
      "Loss at step 37000 : 2.77299618721\n",
      "Loss at step 37500 : 2.81439399719\n",
      "Loss at step 38000 : 2.38104724884\n",
      "Loss at step 38500 : 2.41034221649\n",
      "Loss at step 39000 : 2.2099878788\n",
      "Loss at step 39500 : 2.52982878685\n",
      "Loss at step 40000 : 2.58388352394\n",
      "Loss at step 40500 : 3.20582532883\n",
      "Loss at step 41000 : 2.6782143116\n",
      "Loss at step 41500 : 2.37650918961\n",
      "Loss at step 42000 : 2.69683361053\n",
      "Loss at step 42500 : 2.66032934189\n",
      "Loss at step 43000 : 3.14575815201\n",
      "Loss at step 43500 : 2.31278038025\n",
      "Loss at step 44000 : 3.11481690407\n",
      "Loss at step 44500 : 2.20021653175\n",
      "Loss at step 45000 : 2.97615432739\n",
      "Loss at step 45500 : 2.66393280029\n",
      "Loss at step 46000 : 2.27382826805\n",
      "Loss at step 46500 : 2.4873342514\n",
      "Loss at step 47000 : 2.5584359169\n",
      "Loss at step 47500 : 2.3625421524\n",
      "Loss at step 48000 : 2.54567527771\n",
      "Loss at step 48500 : 4.03465366364\n",
      "Loss at step 49000 : 2.25816011429\n",
      "Loss at step 49500 : 3.11821079254\n",
      "Loss at step 50000 : 2.5367975235\n"
     ]
    }
   ],
   "source": [
    "LOG_DIR = '/tmp/testing/stock2vec_1'\n",
    "#tensorboard --logdir=/tmp/testing/example_2_2\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR, graph=tf.get_default_graph())\n",
    "print_valid_every = 100\n",
    "# Run the skip gram model.\n",
    "valid_words = [1,23,10,300]\n",
    "loss_vec = []\n",
    "loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = generate_batch_data(data, batch_size, window_size)\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "\n",
    "    # Run the train step\n",
    "    _, summary = sess.run([optimizer, merged_summary_op], feed_dict=feed_dict)\n",
    "    \n",
    "    summary_writer.add_summary(summary, i)\n",
    "\n",
    "    # Return the loss\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print(\"Loss at step {} : {}\".format(i+1, loss_val))\n",
    "      \n",
    "    # Validation: Print some random words and top 5 related words\n",
    "#     if (i+1) % print_valid_every == 0:\n",
    "#         sim = sess.run(similarity, feed_dict=feed_dict)\n",
    "#         for j in range(len(valid_words)):\n",
    "#             valid_word = name_dict[word_dictionary_rev[valid_examples[0]]] #word_dictionary_rev[valid_examples[j]]\n",
    "#             top_k = 5 # number of nearest neighbors\n",
    "#             nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "#             #name_dict[word_dictionary_rev[valid_examples[0]]]\n",
    "#             log_str = \"Nearest to {}::\".format(valid_word)\n",
    "#             for k in range(top_k):\n",
    "#                 close_word = name_dict[word_dictionary_rev[valid_examples[k]]] #word_dictionary_rev[nearest[k]]\n",
    "#                 log_str = \"%s %s,\" % (log_str, close_word)\n",
    "#             print(log_str)\n",
    "#             print \"#################\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame() #creates a new dataframe that's empty\n",
    "new_df[\"index\"] = range(num_embedding)\n",
    "import csv\n",
    "new_df.to_csv(os.path.join(LOG_DIR, 'output2.tsv'), sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "# Create randomly initialized embedding weights which will be trained.\n",
    "\n",
    "# Format: tensorflow/tensorboard/plugins/projector/projector_config.proto\n",
    "config = projector.ProjectorConfig()\n",
    "\n",
    "# You can add multiple embeddings. Here we add only one.\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = 'product_vec'\n",
    "# Link this tensor to its metadata file (e.g. labels).\n",
    "embedding.metadata_path = os.path.join(LOG_DIR, 'output2.tsv')\n",
    "\n",
    "# Use the same LOG_DIR where you stored your checkpoint.\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "\n",
    "# The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will\n",
    "# read this file during startup.\n",
    "projector.visualize_embeddings(summary_writer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/testing/stock2vec_1/model.ckpt-1000'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"), 1000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
